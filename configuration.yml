general:
  # If True, then files are read from S3
  aws: False
  # Spark deploy mode (only local and yarn are supported)
  deploy_mode: local
  # Base path of where data is written. Each node appends
  # data to the path: base_path/project_name/node_name/parquet.
  # If running on AWS, the bucket name is appended to the path.
#  base_path: "data"
  base_path: "/Users/jmccartin/Code/AirflowExample/resources/"
  # Force all nodes to regenerate data even if data is present for the period
  force_regen: True

s3:
  bucket: "outra-dev-dataiku-emr"

